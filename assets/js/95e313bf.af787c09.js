"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[321],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(a),m=r,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||i;return a?n.createElement(h,o(o({ref:t},p),{},{components:a})):n.createElement(h,o({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},9399:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(7462),r=(a(7294),a(3905));const i={sidebar_position:1},o="Call for Participation",s={unversionedId:"call_for_participation",id:"call_for_participation",title:"Call for Participation",description:"We are excited to announce the Spoken Language Understanding Grand Challenge @ ICASSP 2023!",source:"@site/docs/call_for_participation.md",sourceDirName:".",slug:"/call_for_participation",permalink:"/spoken_task_oriented_parsing/docs/call_for_participation",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",next:{title:"Semantic Parsing and the STOP Dataset",permalink:"/spoken_task_oriented_parsing/docs/semantic_parsing"}},l={},c=[{value:"Timeline",id:"timeline",level:2},{value:"Contact Us",id:"contact-us",level:2},{value:"Organizers",id:"organizers",level:2}],p={toc:c};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"call-for-participation"},"Call for Participation"),(0,r.kt)("p",null,"We are excited to announce the Spoken Language Understanding Grand Challenge @ ICASSP 2023!"),(0,r.kt)("p",null,"Task oriented conversational assistants are becoming increasingly popular over the past several years. Such Assistants typically allow voice based interactions to complete a variety of tasks around sending messages, getting weather details, and controlling devices, and so forth. Critically, task oriented systems convert a user's utterance into a semantic parse to allow the execution of tasks from natural language input termed as Spoken Language Understanding (SLU). "),(0,r.kt)("p",null,"SLU systems typically consist of first Automatic Speech Recognition (ASR) to convert speech to text and then Natural Language Understanding (NLU) to convert text to a semantic parse. Today many advances in this system consist of independent improvements to ASR and NLU components. However recently there has become an increased interest in End-to-End SLU systems with the promise to improve the performance by leveraging acoustic information lost in the intermediate textual representation and preventing cascading errors from ASR. Further, having one unified model has efficiency advantages when deploying assistant systems on-device for low power / mobile devices. In order to facilitate further progression in the SLU community we release the Spoken Task Oriented Parsing (STOP) dataset ","[1]",". STOP is the largest and most complex publicly available SLU dataset to date. "),(0,r.kt)("p",null,"In this challenge participants are tasked with exploring the SLU space on 3 tracks. (1) Overall quality improvements (2) On-device modeling improvements and (3) Low-resource/Domain Adaptation improvements. We restrict all submissions to open sources models and data to increase accessibility of results. Our dataset and open source baselines are released here: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu"},"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu"),"."),(0,r.kt)("p",null,"We will invite the top-5 teams across all tracks to submit a 2-page paper and present it at ICASSP-2023. Accepted papers will be in the ICASSP proceedings, the review process is coordinated by the challenge organizers and the SPGC chairs. There will be 2 winners in the quality track, 2 in the on-device, and 1 in the low-resource. Selection criteria is described in each respective track."),(0,r.kt)("h2",{id:"timeline"},"Timeline"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"November 28th \u2192 Challenge begins"),(0,r.kt)("li",{parentName:"ul"},"December 21st \u2192 Submissions Open"),(0,r.kt)("li",{parentName:"ul"},"January 24th \u2192 Submission Deadline"),(0,r.kt)("li",{parentName:"ul"},"January 24th to February 3rd \u2192 Review Period"),(0,r.kt)("li",{parentName:"ul"},"February 4th \u2192 Notification of winners"),(0,r.kt)("li",{parentName:"ul"},"February 10th \u2192 Grand Challenge paper acceptance notification"),(0,r.kt)("li",{parentName:"ul"},"February 17th \u2192 Camera-ready Grand challenge papers")),(0,r.kt)("h2",{id:"contact-us"},"Contact Us"),(0,r.kt)("p",null,"For any questions please post on our github issues: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/spoken_task_oriented_parsing/issues"},"https://github.com/facebookresearch/spoken_task_oriented_parsing/issues")," "),(0,r.kt)("h2",{id:"organizers"},"Organizers"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://akshatsh.github.io/"},"Akshat Shrivastava")," (Meta) "),(0,r.kt)("li",{parentName:"ul"},"Paden Tomasello (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Suyoun Kim (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Ali Elkahky (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Daniel Lazar (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Trang Le (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Shan Jiang (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Duc Le (Meta)"),(0,r.kt)("li",{parentName:"ul"},"Aleksandr Livshits (Meta)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.linkedin.com/in/ahmed-aly-1a408514/"},"Ahmed Aly")," (Meta)")),(0,r.kt)("h1",{id:"references"},"References"),(0,r.kt)("p",null,"[1]"," Tomasello, P., A. Shrivastava, D. Lazar, P. chun Hsu, D. Le, A. Sagar, A. M. Elkahky, J. Copet, W.-N. Hsu, Y. Mordechay, R. Algayres, T. Nguyen, E. Dupoux, L. Zettlemoyer, and A. rahman Mohamed (2022). Stop: A dataset for spoken task oriented semantic parsing. ArXiv abs/2207.10643"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2207.10643?context=cs"},"[2207.10643] STOP: A dataset for Spoken Task Oriented Semantic Parsing")))}u.isMDXComponent=!0}}]);