"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[195],{5239:(e,t,a)=>{a.r(t),a.d(t,{default:()=>u});var n=a(7294),r=a(6010),l=a(3285),i=a(9960),s=a(2263),c=a(4996);const o={heroBanner:"heroBanner_UJJx",buttons:"buttons_pzbO",features:"features_keug",featureImage:"featureImage_yA8i"},m=[{title:"Updates",description:n.createElement(n.Fragment,null,n.createElement("ul",null,n.createElement("li",null,"We have extended the deadline for the challenge to Feburary 13th 2023 and will be hosting submissions on ",n.createElement("a",{href:"https://openreview.net/group?id=ICASSP%2F2023%2FGrand_Challenge"},"Open Review"),", check out the ",n.createElement("a",{href:"docs/submissions"},"submissions page")),n.createElement("li",null,"We just announced the call for participation for the Spoken Language Understanding (SLU) challenge @ ICASSP 2023, check out the ",n.createElement("a",{href:"docs/call_for_participation"},"call for participation")),n.createElement("li",null,"Our ",n.createElement("a",{href:"https://arxiv.org/abs/2207.10643?context=cs"},"dataset paper")," was accepted at SLT 2022! "),n.createElement("li",null,"Experiment code is publicly available in ",n.createElement("a",{href:"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu"},"fairseq"),"!")))},{title:"Spoken Langauge Understanding Grand Challenge @ ICASSP 2023",description:n.createElement(n.Fragment,null,"We are excited to announce the Spoken Language Understanding (SLU) Grand Challenge @ ICASSP 2023. Participants of this challenge will compete in spoken language understanding across 3 tracks (1) quality (2) on-device and (3) low-resource and domain adaptation. To get started please head over to ",n.createElement("a",{href:"docs/call_for_participation"},"our page on ICASSP 2023 Grand Challenge!"))},{title:"Dataset Paper",description:n.createElement(n.Fragment,null,"To learn more details about SLU and Spoken Task Oriented Parsing (STOP) dataset. We encourage you to read our paper ",n.createElement("a",{href:"https://arxiv.org/abs/2207.10643?context=cs"},"STOP: A dataset for Spoken Task Oriented Semantic Parsing"),". In order to get started with experimentation: all experiment code, baseline models, and pre-trained checkpoints are available in ",n.createElement("a",{href:"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu"},"fairseq."))}];function d(e){let{imageUrl:t,title:a,description:l}=e;const i=(0,c.Z)(t);return n.createElement("div",{className:(0,r.Z)("col col--4",o.feature)},i&&n.createElement("div",{className:"text--center"},n.createElement("img",{className:o.featureImage,src:i,alt:a})),n.createElement("h3",null,a),n.createElement("p",null,l))}function u(){const e=(0,s.Z)(),{siteConfig:t={}}=e;return n.createElement(l.Z,{title:`${t.title}`,description:"A open source dataset for spoken language understanding <head />"},n.createElement("header",{className:(0,r.Z)("hero hero--primary",o.heroBanner)},n.createElement("div",{className:"container"},n.createElement("h1",{className:"hero__title"},t.title),n.createElement("p",{className:"hero__subtitle"},t.tagline),n.createElement("div",{className:o.buttons},n.createElement(i.Z,{className:(0,r.Z)("button button--outline button--secondary button--lg",o.getStarted),to:(0,c.Z)("docs/call_for_participation")},"Get Started on the Challenge")))),n.createElement("main",null,m&&m.length>0&&n.createElement("section",{className:o.features},n.createElement("div",{className:"container"},n.createElement("div",{className:"row"},m.map((e=>{let{title:t,imageUrl:a,description:r}=e;return n.createElement(d,{key:t,title:t,imageUrl:a,description:r})})))))))}}}]);