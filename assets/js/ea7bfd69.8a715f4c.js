"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[923],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=d(a),m=r,h=c["".concat(s,".").concat(m)]||c[m]||u[m]||l;return a?n.createElement(h,i(i({ref:t},p),{},{components:a})):n.createElement(h,i({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[c]="string"==typeof e?e:r,i[1]=o;for(var d=2;d<l;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8089:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>l,metadata:()=>o,toc:()=>d});var n=a(7462),r=(a(7294),a(3905));const l={sidebar_position:3},i="Challenge Tracks",o={unversionedId:"challenge_tracks",id:"challenge_tracks",title:"Challenge Tracks",description:"Track 1: Quality",source:"@site/docs/challenge_tracks.md",sourceDirName:".",slug:"/challenge_tracks",permalink:"/spoken_task_oriented_parsing/docs/challenge_tracks",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Semantic Parsing and the STOP Dataset",permalink:"/spoken_task_oriented_parsing/docs/semantic_parsing"},next:{title:"Rules",permalink:"/spoken_task_oriented_parsing/docs/rules"}},s={},d=[{value:"Track 1: Quality",id:"track-1-quality",level:2},{value:"Track 2: On-device",id:"track-2-on-device",level:2},{value:"Track 3: Low-resource Domain Adaptation",id:"track-3-low-resource-domain-adaptation",level:2}],p={toc:d};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"challenge-tracks"},"Challenge Tracks"),(0,r.kt)("h2",{id:"track-1-quality"},"Track 1: Quality"),(0,r.kt)("p",null,"The task here is to improve upon the STOP test set. Quality improvements are measured through the Exact Match metric defined in (metrics). "),(0,r.kt)("p",null,"Selection Criteria: We will select 3 winners for this challenge"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Highest EM on the test set using open source methods"),(0,r.kt)("li",{parentName:"ol"},"Second highest EM on the test set using open source methods")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Baselines:")," We present baselines for this challenge in the table below. the results are from the STOP paper ","[1]"," with 3 baseline open source systems (1) HuBERT based Seq2Seq model, (2) Wav2Vec 2.0 based Seq2Seq model, (3) cascaded system with a Wav2Vec 2.0 ASR model and BART-based semantic parser."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Model"),(0,r.kt)("th",{parentName:"tr",align:null},"Exact Match"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Wav2Vec 2.0 ","[1]"),(0,r.kt)("td",{parentName:"tr",align:null},"68.70")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"HuBERT ","[1]"),(0,r.kt)("td",{parentName:"tr",align:null},"69.23")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Cascaded System ","[1]"),(0,r.kt)("td",{parentName:"tr",align:null},"72.36")))),(0,r.kt)("h2",{id:"track-2-on-device"},"Track 2: On-device"),(0,r.kt)("p",null,"A critical use case for end-to-end spoken language understanding is on-device modeling, allowing us to build compressed systems. The goal of this track is to build the highest quality model ",(0,r.kt)("strong",{parentName:"p"},"with a limit of 15 million parameters.")),(0,r.kt)("p",null,"Selection Criteria: "),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Highest EM within 15 million parameters. Exclusively open source models and data."),(0,r.kt)("li",{parentName:"ol"},"Second highest EM within 15 million parameters. Exclusively open source models and data.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Baselines:")," None provided at this parameter count"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Model"),(0,r.kt)("th",{parentName:"tr",align:null},"# Parameters"),(0,r.kt)("th",{parentName:"tr",align:null},"EM"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"N/A"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h2",{id:"track-3-low-resource-domain-adaptation"},"Track 3: Low-resource Domain Adaptation"),(0,r.kt)("p",null,"Due to the multi-domain nature of the STOP dataset and the relevance of domain scaling in language understanding ","[2]",", our third track consists of low-resource domain transfer. Each trained model targets one of the held out domains (weather and reminder)."),(0,r.kt)("p",null,"Given access to the full resource datasets from 6 held-in domains (alarm, event, messaging, music, navigation, timer), as well as a low resource train set from a held out domain (either weather or reminder), train a domain-specific model which achieves the highest EM on the held-out test set (weather or reminder). The low-resource train sets give access to a dataset with 25 samples per intent/slot for training."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Samples Per Intent/Slot (SPIS):")," Samples per intent slot is a data sampling strategy to uniformly sample X samples for each intent and slot. We leverage this measure to create predefined dataset splits for low-resource training that are released alongside the STOP dataset."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Selection Criteria:")," We will select the best model with the highest average EM when trained on the reminder and weather 25 SPIS split. Exclusively open source models and data."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Baselines:"),"  We present baselines for this challenge in the table below from the STOP dataset ","[1]",". The reported HuBERT ","[4]"," model consists of 3 stages. First the HuBERT model is pre-trained with the ASR task on the held-in STOP dataset, second the HuBERT model is further trained on the SLU task on the held-in data, and third the model is adapted to each of the held-out domain (weather 25 SPIS and reminder 25 SPIS)"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Model"),(0,r.kt)("th",{parentName:"tr",align:null},"Reminder 25 SPIS EM"),(0,r.kt)("th",{parentName:"tr",align:null},"Weather 25 SPIS EM"),(0,r.kt)("th",{parentName:"tr",align:null},"Average EM"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"HuBERT ","[1]"),(0,r.kt)("td",{parentName:"tr",align:null},"15.38"),(0,r.kt)("td",{parentName:"tr",align:null},"46.77"),(0,r.kt)("td",{parentName:"tr",align:null},"31.08")))),(0,r.kt)("h1",{id:"references"},"References"),(0,r.kt)("p",null,"[1]"," Tomasello, P., A. Shrivastava, D. Lazar, P. chun Hsu, D. Le, A. Sagar, A. M. Elkahky, J. Copet, W.-N. Hsu, Y. Mordechay, R. Algayres, T. Nguyen, E. Dupoux, L. Zettlemoyer, and A. rahman Mohamed (2022). Stop: A dataset for spoken task oriented semantic parsing. ArXiv abs/2207.10643"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2207.10643?context=cs"},"[2207.10643] STOP: A dataset for Spoken Task Oriented Semantic Parsing")),(0,r.kt)("p",null,"[2]"," Chen, X., A. Ghoshal, Y. Mehdad, L. Zettlemoyer, and S. Gupta (2020). Low-resource domain adaptation for compositional task-oriented semantic parsing. CoRR abs/2010.03546."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2010.03546"},"[2010.03546] Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing")),(0,r.kt)("p",null,"[3]"," Le, D., A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli, and M. L. Seltzer (2022). Deliberation model for on-device spoken language understanding. In INTERSPEECH."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2204.01893"},"[2204.01893] Deliberation Model for On-Device Spoken Language Understanding")),(0,r.kt)("p",null,"[4]"," Hsu, W., B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed (2021). Hubert: Self-supervised speech representation learning by masked prediction of hidden units. CoRR abs/2106.07447."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2106.07447"},"[2106.07447] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units")),(0,r.kt)("p",null,"[5]"," Baevski, A., H. Zhou, A. Mohamed, and M. Auli (2020). wav2vec2.0: A framework for self-supervised learning of speech representations. CoRR abs/2006.11477."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2006.11477"},"[2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations")))}c.isMDXComponent=!0}}]);