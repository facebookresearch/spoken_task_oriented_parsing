"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[347],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),f=a,m=u["".concat(l,".").concat(f)]||u[f]||d[f]||i;return n?r.createElement(m,o(o({ref:t},c),{},{components:n})):r.createElement(m,o({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},8230:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const i={sidebar_position:5},o="Getting Started",s={unversionedId:"getting_started",id:"getting_started",title:"Getting Started",description:"Dataset",source:"@site/docs/getting_started.md",sourceDirName:".",slug:"/getting_started",permalink:"/docs/getting_started",draft:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Rules",permalink:"/docs/rules"},next:{title:"Submissions",permalink:"/docs/submissions"}},l={},p=[{value:"Dataset",id:"dataset",level:2},{value:"Baselines",id:"baselines",level:2},{value:"Running Experiments",id:"running-experiments",level:2}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"getting-started"},"Getting Started"),(0,a.kt)("h2",{id:"dataset"},"Dataset"),(0,a.kt)("p",null,"To download the dataset please proceed to ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu"},"https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu")," to find the download link."),(0,a.kt)("p",null,"The low resource splits can be downloaded from this link: ",(0,a.kt)("a",{parentName:"p",href:"http://dl.fbaipublicfiles.com/stop/low_resource_splits.tar.gz"},"http://dl.fbaipublicfiles.com/stop/low_resource_splits.tar.gz")," "),(0,a.kt)("p",null,"The downloaded data contains both natural speech (",(0,a.kt)("inlineCode",{parentName:"p"},"stop"),") and tts (",(0,a.kt)("inlineCode",{parentName:"p"},"stop_tts"),"). Each contains full resource and low-resource train and validation splits, as well as test splits. Each utterance in the validation and test sets have two recordings, with different speakers (",(0,a.kt)("inlineCode",{parentName:"p"},"eval_0")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"eval_1"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"test_0")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"test_1"),"). The manifests reference audio files across these directories for each dataset split. Each manifest is composed of three files: a ",(0,a.kt)("inlineCode",{parentName:"p"},".tsv")," file referencing the audio files, a ",(0,a.kt)("inlineCode",{parentName:"p"},".ltr")," file with utterance text, and a ",(0,a.kt)("inlineCode",{parentName:"p"},".parse")," file with the corresponding semantic parses for the utterances (labels)."),(0,a.kt)("p",null,"In addition to the manifests, to use fairseq you will need to generate dictionaries for both the source and target text. Do so by running the following command from a fairseq repo:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"./examples/audio_nlp/nlu/create_dict_stop.sh $FAIRSEQ_DATASET_OUTPUT\n")),(0,a.kt)("p",null,"Here ",(0,a.kt)("inlineCode",{parentName:"p"},"$FAIRSEQ_DATASET_OUTPUT")," should point to the location of the ",(0,a.kt)("inlineCode",{parentName:"p"},".ltr")," and ",(0,a.kt)("inlineCode",{parentName:"p"},".parse")," files."),(0,a.kt)("h2",{id:"baselines"},"Baselines"),(0,a.kt)("p",null,"At our ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/blob/main/examples/audio_nlp/nlu/"},"github")," we provide references to several pretrained models as well as their result performance on the test set as a starting point for evaluation."),(0,a.kt)("h2",{id:"running-experiments"},"Running Experiments"),(0,a.kt)("p",null,"In order to run experiments, we provide an example fine-tuning configuration: ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/fairseq/blob/main/examples/audio_nlp/nlu/configs/nlu_finetuning.yaml"},"https://github.com/facebookresearch/fairseq/blob/main/examples/audio_nlp/nlu/configs/nlu_finetuning.yaml")," to reproduce our results. "),(0,a.kt)("p",null,"Example command: "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python fairseq_cli/hydra-train  --config-dir examples/audio_nlp/nlu/configs/  --config-name nlu_finetuning task.data=$FAIRSEQ_DATA_OUTPUT model.w2v_path=$PRETRAINED_MODEL_PATH\n")))}u.isMDXComponent=!0}}]);