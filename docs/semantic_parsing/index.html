<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-semantic_parsing">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Semantic Parsing and the STOP Dataset | STOP: Spoken Task Oriented Parsing</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/semantic_parsing"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Semantic Parsing and the STOP Dataset | STOP: Spoken Task Oriented Parsing"><meta data-rh="true" name="description" content="What is semantic parsing and the annotation format?"><meta data-rh="true" property="og:description" content="What is semantic parsing and the annotation format?"><link data-rh="true" rel="icon" href="/spoken_task_oriented_parsing/img/meta_opensource_logo.svg"><link data-rh="true" rel="canonical" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/semantic_parsing"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/semantic_parsing" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/semantic_parsing" hreflang="x-default"><link rel="stylesheet" href="/spoken_task_oriented_parsing/assets/css/styles.426a5584.css">
<link rel="preload" href="/spoken_task_oriented_parsing/assets/js/runtime~main.49a82210.js" as="script">
<link rel="preload" href="/spoken_task_oriented_parsing/assets/js/main.800ba92b.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/spoken_task_oriented_parsing/"><b class="navbar__title text--truncate">Spoken Task Oriented Parsing</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/spoken_task_oriented_parsing/docs/call_for_participation">ICASSP 2023 Grand Challenge</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebookresearch/spoken_task_oriented_parsing" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/call_for_participation">Call for Participation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/spoken_task_oriented_parsing/docs/semantic_parsing">Semantic Parsing and the STOP Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/challenge_tracks">Challenge Tracks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/rules">Rules</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/getting_started">Getting Started</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/submissions">Submissions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/contact_us">Contact Us</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/spoken_task_oriented_parsing/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Semantic Parsing and the STOP Dataset</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Semantic Parsing and the STOP Dataset</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-semantic-parsing-and-the-annotation-format">What is semantic parsing and the annotation format?<a class="hash-link" href="#what-is-semantic-parsing-and-the-annotation-format" title="Direct link to heading">​</a></h2><p>Semantic parsing is the task of converting a user’s request into a structured format for executing tasks. Examples are given in the table below from the STOP dataset. Each parse consists of 2 key components: Intent and Slot.  As an example consider <code>“what is the weather in seattle”</code></p><ul><li><p><strong>Intent:</strong> Intention of a user based on predefined intent labels (<code>intent = get weather</code>)</p></li><li><p><strong>Slot:</strong> Slots correspond to relevant arguments for each intent (<code>slot = {location: weather}</code>)</p></li><li><p><strong>Compositionality:</strong> As queries continue to grow more complex, intent and slot filling as a flat structure no longer suffices. Consider “Directions to the Eagles Game” in order to parse this example, our meaning representation encompasses nested intents and slots. An example is depicted in the figure below where This query can be phrased with nested intents and slots to provide a granular definition of the users intent.</p></li></ul><p><img loading="lazy" alt="Example Parse" src="/spoken_task_oriented_parsing/assets/images/example_parse_stop-f607a5a2e106e5b07544f484158f2459.png" width="854" height="562" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="task-definition">Task Definition<a class="hash-link" href="#task-definition" title="Direct link to heading">​</a></h2><p>Concretely in this task, you are asked to produce the semantic parse as a linearized tree structure (Output in the table below), given an audio sample (input in the table below). Our baseline system <!-- -->[1]<!-- --> proposes this as a sequence-to-sequence task leveraging HuBERT <!-- -->[4]<!-- --> and Wav2Vec 2.0 <!-- -->[5]<!-- --> as the underlying encoder representations with a transformer decoder. </p><table><thead><tr><th>Input</th><th>Output</th></tr></thead><tbody><tr><td>Audio: Directions to the Eagles Game</td><td>[IN:GET_DIRECTIONS [SL:DESTINATION [IN:GET_EVENT <!-- -->[SL:NAME_EVENT Eagles ][SL:CAT_EVENT game ]<!-- --> ] ] ]</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="metrics">Metrics<a class="hash-link" href="#metrics" title="Direct link to heading">​</a></h2><p><strong>Exact Match</strong>: We compute the exact match of the predicted output and the annotated parse in the dataset comparing the intent/slot labels as well as leaf slot text. </p><p><img loading="lazy" alt="Example Metric" src="/spoken_task_oriented_parsing/assets/images/exact_match_example-9164af3fd122f68b2efdb1573e623fa0.png" width="752" height="378" class="img_ev3q"></p><p>In the examples above we show examples of correct and incorrect semantic parses for the utterance ``what is the weather in seattle at 6pm&#x27;&#x27;. The first example is correct because the two parses match exactly in output. The second parse is incorrect because the slot text (seattle vs sf) is different. The third parse is incorrect because seattle was incorrectly labeled as a date time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="stop-dataset">STOP Dataset<a class="hash-link" href="#stop-dataset" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-statistics">Dataset Statistics<a class="hash-link" href="#dataset-statistics" title="Direct link to heading">​</a></h3><p>The STOP (spoken task oriented parsing) dataset is the largest most semantically complex end-to-end spoken language dataset. It contains over 200000 audio files from over 800 different speakers. The text utterances and semantic parses are taken from TOPv2 <!-- -->[2]<!-- --> which contains 125K unique utterance parse pairs, across 8 different domains, alarm, event, messaging, music, navigation, reminder, timer and weather. Crowd workers were requested to record themselves speaking each utterance through Amazon&#x27;s Mechanical Turk. Two audio recordings were taken of every utterance in the validation and test set, and one for each utterance in the train set. The semantic parse may contain nested intents or in other words compositional queries.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="low-resource-splits">Low Resource Splits<a class="hash-link" href="#low-resource-splits" title="Direct link to heading">​</a></h3><p>Along with the overall dataset, the STOP dataset supplies additional dataset splits for low-resource learning. Specifically, the STOP dataset provides low-resource train and validation sets for the “reminder” and “weather” domains, each with 25 Samples Per Intent-Slot (SPIS) <!-- -->[2]<!-- -->. The construction of these low-resource sets ensures that each intent and slot type is present in the low-resource train set in at least 25 samples. The reminder and weather train sets have 480 and 162 samples, respectively, and their validation sets have 328 and 139 samples, respectively.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="license">LICENSE<a class="hash-link" href="#license" title="Direct link to heading">​</a></h3><p>The license for the dataset can be found here: <a href="https://dl.fbaipublicfiles.com/stop/LICENSE.txt" target="_blank" rel="noopener noreferrer">https://dl.fbaipublicfiles.com/stop/LICENSE.txt</a> </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="citation">Citation<a class="hash-link" href="#citation" title="Direct link to heading">​</a></h2><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{stop2022,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  author    = {Paden Tomasello and Akshat Shrivastava and Daniel Lazar and Po-Chun Hsu and Duc Le and Adithya Sagar and Ali Elkahky and Jade Copet and Wei-Ning Hsu and Yossef Mordechay and Robin Algayres and Tu Anh Nguyen and Emmanuel Dupoux and Luke Zettlemoyer and Abdelrahman Mohamed},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  title     = {{STOP: A dataset for Spoken Task Oriented Semantic Parsing}},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  booktitle   = {CoRR},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  eprinttype = {arXiv},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>References</h1><p>[1]<!-- --> Tomasello, P., A. Shrivastava, D. Lazar, P. chun Hsu, D. Le, A. Sagar, A. M. Elkahky, J. Copet, W.-N. Hsu, Y. Mordechay, R. Algayres, T. Nguyen, E. Dupoux, L. Zettlemoyer, and A. rahman Mohamed (2022). Stop: A dataset for spoken task oriented semantic parsing. ArXiv abs/2207.10643</p><p><a href="https://arxiv.org/abs/2207.10643?context=cs" target="_blank" rel="noopener noreferrer">[2207.10643] STOP: A dataset for Spoken Task Oriented Semantic Parsing</a></p><p>[2]<!-- --> Chen, X., A. Ghoshal, Y. Mehdad, L. Zettlemoyer, and S. Gupta (2020). Low-resource domain adaptation for compositional task-oriented semantic parsing. CoRR abs/2010.03546.</p><p><a href="https://arxiv.org/abs/2010.03546" target="_blank" rel="noopener noreferrer">[2010.03546] Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing</a></p><p>[3]<!-- --> Le, D., A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli, and M. L. Seltzer (2022). Deliberation model for on-device spoken language understanding. In INTERSPEECH.</p><p><a href="https://arxiv.org/abs/2204.01893" target="_blank" rel="noopener noreferrer">[2204.01893] Deliberation Model for On-Device Spoken Language Understanding</a></p><p>[4]<!-- --> Hsu, W., B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed (2021). Hubert: Self-supervised speech representation learning by masked prediction of hidden units. CoRR abs/2106.07447.</p><p><a href="https://arxiv.org/abs/2106.07447" target="_blank" rel="noopener noreferrer">[2106.07447] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p><p>[5]<!-- --> Baevski, A., H. Zhou, A. Mohamed, and M. Auli (2020). wav2vec2.0: A framework for self-supervised learning of speech representations. CoRR abs/2006.11477.</p><p><a href="https://arxiv.org/abs/2006.11477" target="_blank" rel="noopener noreferrer">[2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/spoken_task_oriented_parsing/docs/call_for_participation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Call for Participation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/spoken_task_oriented_parsing/docs/challenge_tracks"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Challenge Tracks</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-semantic-parsing-and-the-annotation-format" class="table-of-contents__link toc-highlight">What is semantic parsing and the annotation format?</a></li><li><a href="#task-definition" class="table-of-contents__link toc-highlight">Task Definition</a></li><li><a href="#metrics" class="table-of-contents__link toc-highlight">Metrics</a></li><li><a href="#stop-dataset" class="table-of-contents__link toc-highlight">STOP Dataset</a><ul><li><a href="#dataset-statistics" class="table-of-contents__link toc-highlight">Dataset Statistics</a></li><li><a href="#low-resource-splits" class="table-of-contents__link toc-highlight">Low Resource Splits</a></li><li><a href="#license" class="table-of-contents__link toc-highlight">LICENSE</a></li></ul></li><li><a href="#citation" class="table-of-contents__link toc-highlight">Citation</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://opensource.fb.com/legal/privacy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/terms/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/data-policy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Data Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/cookie-policy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Cookie Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://opensource.fb.com" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/spoken_task_oriented_parsing/img/meta_opensource_logo_negative.svg" alt="Meta Open Source Logo" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/spoken_task_oriented_parsing/img/meta_opensource_logo_negative.svg" alt="Meta Open Source Logo" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"></a></div><div class="footer__copyright">Copyright © 2022 Meta Platforms, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/spoken_task_oriented_parsing/assets/js/runtime~main.49a82210.js"></script>
<script src="/spoken_task_oriented_parsing/assets/js/main.800ba92b.js"></script>
</body>
</html>