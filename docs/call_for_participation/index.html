<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-call_for_participation">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Call for Participation | STOP: Spoken Task Oriented Parsing</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/call_for_participation"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Call for Participation | STOP: Spoken Task Oriented Parsing"><meta data-rh="true" name="description" content="We are excited to announce the Spoken Language Understanding Grand Challenge @ ICASSP 2023!"><meta data-rh="true" property="og:description" content="We are excited to announce the Spoken Language Understanding Grand Challenge @ ICASSP 2023!"><link data-rh="true" rel="icon" href="/spoken_task_oriented_parsing/img/meta_opensource_logo.svg"><link data-rh="true" rel="canonical" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/call_for_participation"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/call_for_participation" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/spoken_task_oriented_parsing/docs/call_for_participation" hreflang="x-default"><link rel="stylesheet" href="/spoken_task_oriented_parsing/assets/css/styles.426a5584.css">
<link rel="preload" href="/spoken_task_oriented_parsing/assets/js/runtime~main.101dfd6f.js" as="script">
<link rel="preload" href="/spoken_task_oriented_parsing/assets/js/main.800ba92b.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/spoken_task_oriented_parsing/"><b class="navbar__title text--truncate">Spoken Task Oriented Parsing</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/spoken_task_oriented_parsing/docs/call_for_participation">ICASSP 2023 Grand Challenge</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebookresearch/spoken_task_oriented_parsing" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/spoken_task_oriented_parsing/docs/call_for_participation">Call for Participation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/semantic_parsing">Semantic Parsing and the STOP Dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/challenge_tracks">Challenge Tracks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/rules">Rules</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/getting_started">Getting Started</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/submissions">Submissions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/spoken_task_oriented_parsing/docs/contact_us">Contact Us</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/spoken_task_oriented_parsing/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Call for Participation</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Call for Participation</h1><p>We are excited to announce the Spoken Language Understanding Grand Challenge @ ICASSP 2023!</p><p>Task oriented conversational assistants are becoming increasingly popular over the past several years. Such Assistants typically allow voice based interactions to complete a variety of tasks around sending messages, getting weather details, and controlling devices, and so forth. Critically, task oriented systems convert a user&#x27;s utterance into a semantic parse to allow the execution of tasks from natural language input termed as Spoken Language Understanding (SLU). </p><p>SLU systems typically consist of first Automatic Speech Recognition (ASR) to convert speech to text and then Natural Language Understanding (NLU) to convert text to a semantic parse. Today many advances in this system consist of independent improvements to ASR and NLU components. However recently there has become an increased interest in End-to-End SLU systems with the promise to improve the performance by leveraging acoustic information lost in the intermediate textual representation and preventing cascading errors from ASR. Further, having one unified model has efficiency advantages when deploying assistant systems on-device for low power / mobile devices. In order to facilitate further progression in the SLU community we release the Spoken Task Oriented Parsing (STOP) dataset <!-- -->[1]<!-- -->. STOP is the largest and most complex publicly available SLU dataset to date. </p><p>In this challenge participants are tasked with exploring the SLU space on 3 tracks. (1) Overall quality improvements (2) On-device modeling improvements and (3) Low-resource/Domain Adaptation improvements. We restrict all submissions to open sources models and data to increase accessibility of results. Our dataset and open source baselines are released here: <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/fairseq/tree/main/examples/audio_nlp/nlu</a>.</p><p>We will invite the top-5 teams across all tracks to submit a 2-page paper and present it at ICASSP-2023. Accepted papers will be in the ICASSP proceedings, the review process is coordinated by the challenge organizers and the SPGC chairs. There will be 2 winners in the quality track, 2 in the on-device, and 1 in the low-resource. Selection criteria is described in each respective track.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="timeline">Timeline<a class="hash-link" href="#timeline" title="Direct link to heading">​</a></h2><p>All deadlines are Anywhere On Earth (AOE).</p><ul><li>November 28th, 2022 → Challenge begins</li><li>December 21st, 2022 → Submissions Open</li><li>January 24th, 2023 → Submission Deadline</li><li>January 24th, 2023 to February 3rd, 2023 → Review Period</li><li>February 4th, 2023 → Notification of winners</li><li>February 10th, 2023 → Grand Challenge paper acceptance notification</li><li>February 17th, 2023 → Camera-ready Grand challenge papers</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact-us">Contact Us<a class="hash-link" href="#contact-us" title="Direct link to heading">​</a></h2><p>For any questions please post on our github issues: <a href="https://github.com/facebookresearch/spoken_task_oriented_parsing/issues" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/spoken_task_oriented_parsing/issues</a> </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="organizers">Organizers<a class="hash-link" href="#organizers" title="Direct link to heading">​</a></h2><ul><li><a href="https://akshatsh.github.io/" target="_blank" rel="noopener noreferrer">Akshat Shrivastava</a> (Meta) </li><li>Paden Tomasello (Meta)</li><li>Suyoun Kim (Meta)</li><li>Ali Elkahky (Meta)</li><li>Daniel Lazar (Meta)</li><li>Trang Le (Meta)</li><li>Shan Jiang (Meta)</li><li>Duc Le (Meta)</li><li>Aleksandr Livshits (Meta)</li><li><a href="https://www.linkedin.com/in/ahmed-aly-1a408514/" target="_blank" rel="noopener noreferrer">Ahmed Aly</a> (Meta)</li></ul><h1>References</h1><p>[1]<!-- --> Tomasello, P., A. Shrivastava, D. Lazar, P. chun Hsu, D. Le, A. Sagar, A. M. Elkahky, J. Copet, W.-N. Hsu, Y. Mordechay, R. Algayres, T. Nguyen, E. Dupoux, L. Zettlemoyer, and A. rahman Mohamed (2022). Stop: A dataset for spoken task oriented semantic parsing. ArXiv abs/2207.10643</p><p><a href="https://arxiv.org/abs/2207.10643?context=cs" target="_blank" rel="noopener noreferrer">[2207.10643] STOP: A dataset for Spoken Task Oriented Semantic Parsing</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="citation">Citation<a class="hash-link" href="#citation" title="Direct link to heading">​</a></h2><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{stop2022,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  author    = {Paden Tomasello and Akshat Shrivastava and Daniel Lazar and Po-Chun Hsu and Duc Le and Adithya Sagar and Ali Elkahky and Jade Copet and Wei-Ning Hsu and Yossef Mordechay and Robin Algayres and Tu Anh Nguyen and Emmanuel Dupoux and Luke Zettlemoyer and Abdelrahman Mohamed},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  title     = {{STOP: A dataset for Spoken Task Oriented Semantic Parsing}},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  booktitle   = {CoRR},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  eprinttype = {arXiv},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/spoken_task_oriented_parsing/docs/semantic_parsing"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Semantic Parsing and the STOP Dataset</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#timeline" class="table-of-contents__link toc-highlight">Timeline</a></li><li><a href="#contact-us" class="table-of-contents__link toc-highlight">Contact Us</a></li><li><a href="#organizers" class="table-of-contents__link toc-highlight">Organizers</a></li><li><a href="#citation" class="table-of-contents__link toc-highlight">Citation</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://opensource.fb.com/legal/privacy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/terms/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Terms<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/data-policy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Data Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://opensource.fb.com/legal/cookie-policy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Cookie Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://opensource.fb.com" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/spoken_task_oriented_parsing/img/meta_opensource_logo_negative.svg" alt="Meta Open Source Logo" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/spoken_task_oriented_parsing/img/meta_opensource_logo_negative.svg" alt="Meta Open Source Logo" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"></a></div><div class="footer__copyright">Copyright © 2022 Meta Platforms, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/spoken_task_oriented_parsing/assets/js/runtime~main.101dfd6f.js"></script>
<script src="/spoken_task_oriented_parsing/assets/js/main.800ba92b.js"></script>
</body>
</html>